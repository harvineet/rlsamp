"""
Adapted from code by Ian Osband
https://github.com/iosband/ts_tutorial/

Finite armed bandit environments."""

from __future__ import division
from __future__ import print_function

from functools import partial

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from base.environment import Environment

##############################################################################

def read_data(filepath):
  data = pd.read_csv(filepath, delimiter=',')
  data.iloc[:,123] = data.iloc[:,123].map({1:-1,2:1}) # change labels 1/2 to -1/1
  scaler = StandardScaler()
  scaler.fit(data.iloc[:,0:123])
  data.iloc[:,0:123] = scaler.transform(data.iloc[:,0:123])
  return data

def get_context_data_file(data, ts):
  return data.iloc[ts-1,0:123].values # ts is indexed from 1

def get_label_data_file(data, ts):
  return data.iloc[ts-1,123]

def sinewave(ts):
  return [0.1, np.sin(2*np.pi*5*(ts-1)/10000)] + np.random.normal(loc=[0.0,0.0], scale=0.1)

def normal_iid(ts):
  return np.random.normal(loc=[0.0, 0.0], scale=0.1)

def uniform_iid(ts):
  return np.random.uniform(low=-0.5, high=0.5, size=100)

def linear_classifier(x, theta):
  margin = np.dot(x, theta)
  prob = (1+margin)/2
  label = 2 * np.random.binomial(1, p=prob) - 1.0 # output -1,1 from Bernoulli noise model
  return label
  # return np.sign(margin) # noise-free

class ContextualBanditDataFileContext(Environment):
  """Simple linear contextual bandit with contexts read from a file.
  E[y(t)|x(t)] = theta*x(t), x(t) = f(t) + eps.
  Action: (a(t),y^(t)), a(t): 0 or 1 for sample or not.
  contextfn: x(t), gives numpy array output"""

  def __init__(self, infile, sample_cost=0.0):
    data = read_data(infile)
    self.contextfn = partial(get_context_data_file, data)
    self.true_model = partial(get_label_data_file, data)
    self.sample_cost = sample_cost

    self.step = 1 # time step for y(t), x(t)
    self.x_t = self.contextfn(self.step)
    self.y_t = None
    self.optimal_reward = 1

    self.num_query = 0

  def get_observation(self):
    self.x_t = self.contextfn(self.step)
    # if np.abs(np.dot(self.x_t, self.true_theta))>1:
    #   print('Error margin greater than 1', np.dot(self.x_t, self.true_theta))
    #   return self.get_observation()
    self.y_t = self.true_model(self.step) # sample label but do not return
    # print(self.step, self.x_t, self.y_t)
    return self.x_t

  def get_optimal_reward(self):
    return 1 # correct y prediction

  def get_expected_reward(self, action):
    query, pred = action
    exp_reward = -1 * self.sample_cost * int(query!=0) + int(self.y_t * pred > 0) # 1 - 0/1 loss
    return exp_reward

  def get_stochastic_reward(self, action):
    assert len(action)==2
    query, _ = action
    if query==0:
      reward = None # not sampled
    else:
      reward = self.y_t # true label
    return reward

  def advance(self, action, reward):
    """Updating the environment (useful for nonstationary bandit)."""
    # if np.abs(np.dot(self.x_t, self.true_theta)) > 1:
    #   print("Error: Norm of theta exceeds 1. Reduce true theta", self.x_t, self.true_theta, np.dot(self.x_t, self.true_theta))

    self.step+=1

    self.num_query+=action[0]

class ContextualBanditFunctionalContext(Environment):
  """Simple linear contextual bandit with noisy contexts generated by given functions.
  E[y(t)|x(t)] = theta*x(t), x(t) = f(t) + eps.
  Action: (a(t),y^(t)), a(t): 0 or 1 for sample or not.
  contextfn: x(t), gives numpy array output"""

  def __init__(self, contextfn, true_theta, true_model, sample_cost=0.0):
    self.contextfn = contextfn
    self.true_theta = np.array(true_theta)
    self.true_model = true_model
    self.sample_cost = sample_cost

    self.step = 1 # time step for y(t), x(t)
    self.x_t = self.contextfn(self.step)
    self.y_t = None
    self.optimal_reward = 1

    self.num_query = 0

  def get_observation(self):
    self.x_t = self.contextfn(self.step)
    if np.abs(np.dot(self.x_t, self.true_theta))>1:
      print('Error margin greater than 1', np.dot(self.x_t, self.true_theta))
      return self.get_observation()
    self.y_t = self.true_model(self.x_t, self.true_theta) # sample label but do not return
    return self.x_t

  def get_optimal_reward(self):
    pred_opt = np.sign(np.dot(self.x_t, self.true_theta))
    return int(self.y_t * pred_opt > 0) # no sampling cost plus 1 - 0/1 loss with true_theta

  def get_expected_reward(self, action):
    query, pred = action
    exp_reward = -1 * self.sample_cost * int(query!=0) + int(self.y_t * pred > 0) # 1 - 0/1 loss
    return exp_reward

  def get_stochastic_reward(self, action):
    assert len(action)==2
    query, _ = action
    if query==0:
      reward = None # not sampled
    else:
      reward = self.y_t # true label
    return reward

  def advance(self, action, reward):
    """Updating the environment (useful for nonstationary bandit)."""
    # if np.abs(np.dot(self.x_t, self.true_theta)) > 1:
    #   print("Error: Norm of theta exceeds 1. Reduce true theta", self.x_t, self.true_theta, np.dot(self.x_t, self.true_theta))

    self.step+=1

    self.num_query+=action[0]

class FiniteArmedBernoulliBandit(Environment):
  """Simple N-armed bandit."""

  def __init__(self, probs):
    self.probs = np.array(probs)
    assert np.all(self.probs >= 0)
    assert np.all(self.probs <= 1)

    self.optimal_reward = np.max(self.probs)
    self.n_arm = len(self.probs)

  def get_observation(self):
    return self.n_arm

  def get_optimal_reward(self):
    return self.optimal_reward

  def get_expected_reward(self, action):
    return self.probs[action]

  def get_stochastic_reward(self, action):
    return np.random.binomial(1, self.probs[action])
